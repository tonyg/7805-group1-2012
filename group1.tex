\documentclass[english]{article}
\usepackage{mathpazo}
\usepackage{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=3cm,bmargin=4cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsthm}
\usepackage{amsmath}

\makeatletter
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{plain}
\newtheorem{lem}[thm]{Lemma}
\makeatother

\usepackage{babel}

%%\setlength{\parindent}{0pt}
%%\setlength{\parskip}{2ex}

\begin{document}

\title{7805 Class project, 2012: Group 1}
\author{
  Tony Garnock-Jones,
  Zahra Jafargholi,
  Ravishankar Rajagopal,\\
  Bochao Shen,
  Saber Shokat Fadaee,
%%  Triết Võ Hữu -- I tried to get inputenc to accept this but it wouldn't
  Triet Vo Huu
}
\maketitle

\section{Overview}

We show that any function computable in time $T(n)$ on a Random-Access
Computer (RAC, defined formally below) is computable in time
$T(n)(\log T(n))^{O(1)}$ on a $k$-tape nondeterministic
sequential-access Turing machine. We provide a chain of simulating
machines, each of which efficiently simulates the preceding machine,
leading from our initial random-access machine model to our final
sequential-access model.

\begin{defn}
  %% Verbatim text retyped from Gurevich:
  %%
  %% ``... one often checks that any T(n)-time-bounded machine of one
  %% kind can be simulated by some machine of the other kind in time
  %% T(n)h(n) where the overhead h(n) is bounded by a polynomial of
  %% T(n) or even n. ... It is often the case that the overhead h(n)
  %% is bounded by polynomial of the logarithm of T(n); let us call
  %% such simulations efficient.''

  (Efficient simulation, after \cite{DBLP:conf/ershov/GurevichS89}.) A
  machine of some kind that is $T(n)$-time-bounded is said to be
  \emph{efficiently simulated} by some machine of another kind when
  the simulating machine is time-bounded by a polynomial of $\log
  T(n)$; that is, when the overhead of simulation is bounded by a
  polylog function of $n$.
\end{defn}

Although we start from a specific machine, RAC, it is shown in
\cite{DBLP:conf/ershov/GurevichS89} that a number of variants,
including random-access Turing machines, Kolmogorov machines and
Sch\"{o}nhage machines, all efficiently simulate each other and
therefore share the same notion of $n^k$ computability,
$k\in\mathbb{N}$.

\subsection*{Outline}

We first define the machine models we will be using: random-access
computers, random-access Turing machines (RTMs), and $k$-tape
sequential-access Turing machines. We then show that every specific
instance of the RAC model can be simulated by an RTM, and furthermore
that the simulating RTM's behavior is bounded in certain ways that
will be important to the proof of our main claim. Finally, we show
that any computation performed by an RTM can be efficiently simulated
on a $k$-tape \emph{non-deterministic} Turing machine, where $k>1$. In
order to accomplish this, we prove and make use of the fact that a
non-deterministic Turing machine can sort in time $n \log n$, where
$n$ is the length of the input.

\section{Definitions: RAC, $\ell$-bit RTM, $k$-tape TM}

\begin{defn}
  (Random-access Computer, RAC.) A random-access computer is, quoting
  \cite{DBLP:conf/ershov/GurevichS89}, ``an abstract machine with a
  sequence of memory locations'', where the size of each memory cell
  and the number of such cells varies with the size of the input to
  the machine. In particular, each memory cell is $\ell$ bits wide,
  where $\ell=c \log n$ and $n$ is the size of the input. Furthermore,
  there are exactly $2^\ell$ memory cells available, meaning that the
  size of each cell is exactly large enough to represent any address
  in the machine's memory space.

  RACs are similar in spirit to the CPUs we use in our day-to-day work
  with desktop computers. Time in an RAC is measured in terms of the
  number of instructions executed. In order to be precise about the
  simulations we will be defining, we choose to pin down a few details
  omitted in \cite{DBLP:conf/ershov/GurevichS89}:

  \begin{itemize}
    \item Our RAC will have a fixed number of $\ell$-bit registers.

    \item The instructions in our RAC will be \textsc{Load},
      \textsc{Store}, and the usual register-to-register arithmetic
      operations and control transfers. Our primary concern will be
      with memory accesses, so the exact instruction set other than
      loads and stores is not important.
  \end{itemize}
\end{defn}

\begin{defn}
  ($\ell$-bit Random-access Turing Machine, $\ell$-bit RTM.) We use
  verbatim\footnote{There are a couple of typos in Gurevich's paper;
    we have corrected them in the presentation here.} the definition
  of an RTM from \cite{DBLP:conf/ershov/GurevichS89}:

  \begin{quote}
    An RTM is a Turing machine with three linear tapes called the
    \emph{main tape}, the \emph{address tape} and the \emph{auxiliary
      tape}, such that the head of the main tape (the \emph{main
      head}) is always on the cell whose number is the contents of the
    address tape. An instruction for an RTM has the form
    \begin{equation*}
      (p,\alpha_1,\alpha_2,\alpha_3) \rightarrow
      (q,\beta_1,\beta_2,\beta_3,\gamma_1,\gamma_2)
    \end{equation*}
    and means the following: If the control state is $p$ and the
    symbols in the observed cells on the three tapes are binary digits
    $\alpha_1,\alpha_2,\alpha_3$ respectively, then print binary
    digits $\beta_1,\beta_2,\beta_3$ in the respective cells, move the
    address head to the left (resp. to the right) if $\gamma_1=-1$
    (resp. $\gamma_1=1$), move the auxiliary-tape head with respect
    to $\gamma_2$ and go to control state $q$.
  \end{quote}

  We extend Gurevich and Shelah's definition with the constraint that
  both the address and the auxiliary tapes are bounded so as to be no
  longer than $O(\ell)$ bits each.\footnote{Note that this is
    different to the class of \emph{frugal} RTMs, defined as part of
    the chain of machines in \cite{DBLP:conf/ershov/GurevichS89}. The
    lengths of the address and auxiliary tapes in a frugal RTM are
    bounded by $O(\log t)$ at every moment in the operation of the
    machine; we do not rely on such a strict bound here, so do not
    further consider frugal RTMs as defined by Gurevich and Shelah.}
\end{defn}

\begin{defn}
  ($k$-tape sequential-access Turing machines, $k$-tape TMs.) A
  $k$-tape TM is a Turing machine with $k$ linear tapes, each of which
  has a corresponding head, which is constrained to move one cell at a
  time along its tape, just like a normal single-tape Turing
  machine. Instructions for $k$-tape TMs have the form
  \begin{equation*}
    (p,\alpha_1,\alpha_2,...\alpha_k) \rightarrow
    (q,\beta_1,\beta_2,...\beta_k,\gamma_1,\gamma_2,...\gamma_k)
  \end{equation*}
  where $p$ is the control state, the $\alpha$s are the contents of
  the cells under the heads of the $k$ tapes, $q$ is the resulting
  control state, the $\beta$s are to be written into the cells under
  the heads, and the $\gamma$s control each head's movements in the
  same way as they do for RTMs as defined above.
\end{defn}

\section{Efficient simulation of RACs using $\ell$-bit RTMs}

\begin{lem}
  RACs with $\ell$-bit memory cells can be efficiently simulated by
  $\ell$-bit RTMs.
\end{lem}

\begin{proof}
  ((I'll write this up ---tonyg))
\end{proof}

\section{Efficient simulation of $\ell$-bit RTMs using non-deterministic $k$-tape TMs}

Simulating a computation on an $\ell$-bit RTM using a multi-tape TM is
carried out by non-deterministically guessing the sequence of steps
involved in the computation and then checking the correctness of the
guess. However in order to do the latter, we need multi-tape TMs to be
able to sort efficiently.

\subsection{Efficient sorting}

\begin{lem}
  Let $L$ be a list containing $|L|$ natural numbers, and let $C(L)$
  be an encoding of $L$, of length $|C(L)|$ symbols. Then, $L$ can be
  sorted by a $k$-tape TM in time $O(|C(L)|\log|L|)$.
\end{lem}

\begin{proof}
  In the following let $[n] = \{1,2,\ldots n\}$. Let $L = (l_i \in N:
  i \in [m])$ be a list (sequence) of natural numbers $(l_1,l_2,
  \ldots l_m)$. Then the encoding $C(L)$ $\in \Sigma^*$ is defined as
\[
C(L) = c(l_1) \# c(l_2) \# \ldots c(l_m) \#
\]
  where $\Sigma$, the input alphabet to our sorting machine, is the
  set including $0$ and $1$ as well as the pound symbol ($\#$), and
  $c(v)$ is the binary representation of $v \in \mathbb{N}$. For
  example, $C((1,2,13))=1\#10\#1101\#$.

  We define $\Gamma$, our tape alphabet, to be $\Sigma$ with the
  addition of the symbol comma ($,$). We proceed using an algorithm
  based on merge sort, where we repeatedly merge adjacent sorted
  sublists until just one list remains. At the $i$th iteration over
  the input, the sublists we consider have length $2^{i-1}$.

  We choose to use three tapes: the first, main, tape holds the input
  and the partial outputs at each iteration, and the second and third
  tapes are used as working space to hold the sublists to be merged. \\

  The operation of the machine in each iteration is as follows:

  \begin{enumerate}
    \item \label{iteration-start} Return the heads to the beginnings
      of the tapes.
    \item \label{process-sublist} If we are at the end of the main
      tape, then
      \begin{itemize}
        \item if we haven't yet processed any sublists during this
          iteration, the original input was empty. Halt, since an
          empty list is always sorted.
        \item otherwise, go to step \ref{iteration-start}.
      \end{itemize}
    \item Copy symbols from the main tape to the second tape until a
      pound symbol is seen.
    \item If the end of the tape follows the pound symbol we just saw,
      then
      \begin{itemize}
        \item if this was the first sublist we processed during this
          iteration, then we are done: the input has been
          sorted. Halt.
        \item otherwise, we have an odd number of sublists. The final
          iteration of the algorithm will take care of this
          case. Return to step \ref{iteration-start}.
      \end{itemize}
    \item Otherwise, copy symbols from the main tape to the third tape
      until a pound symbol is seen.
    \item Return the heads of the second and third tapes to the
      beginning, and move the head of the main tape back over the two
      sublists just copied to the first symbol of the first sublist of
      the two.
    \item \label{merge-loop} If the symbol on the second tape is the
      pound symbol, place a comma on the main tape and then copy
      symbols from the third tape onto the main tape until we see a
      pound symbol on the third tape also. Place a pound symbol on the
      main tape and go to step \ref{process-sublist}.
    \item Otherwise, if the symbol on the third tape is the pound
      symbol, place a comma on the main tape and then copy symbols
      from the second tape onto the main tape until we see a pound
      symbol on the second tape also. Place a pound symbol on the main
      tape and go to step \ref{process-sublist}.
    \item Otherwise, compare the encodings of the numbers under the
      heads of the second and third tapes. If the smaller is the
      element from the second tape, copy it to the main tape;
      otherwise, copy the element from the third tape. Place a comma
      on the main tape. Rewind whichever of the tapes did not have an
      element copied from it so that its element will be compared
      again next time round. Go to step \ref{merge-loop}.
  \end{enumerate}

  If there are an even number of pound-symbol-delimited sublists in
  the input, then they are of equal length at the start of each
  iteration; and if there are an odd number in the input, then all but
  one will have equal length. To see this, consider that as the
  algorithm begins, all sublists are of length 1, and as it proceeds,
  each pair of length $x$ sublists is replaced by a single length $2x$
  sublist, except for any odd-man-out at the end of the input, which
  will not be processed until the final iteration.

  It remains to be shown that the machine takes time
  $O(|C(L)|\log|L|)$. We can see that there will be $log_2|L|$
  iterations, since each iteration halves the number of remaining
  sublists until a single sublist remains. Each iteration will take
  $O(|C(L)|)$ steps, since every element of the input is examined on
  each iteration, and $O(1)$ steps are needed to perform the necessary
  copyings and comparisons for each element of the input. Combining
  these two observations gives us the required bound.

%% Each comparison process goes as follows: Based on the count of the
%% stages from the fourth tape we copy the first $2^{n-1}$ onto tape two
%% and the next $2^{n-1}$ elements onto tape three. Then we compare the
%% first two elements on the two tapes bit by bit and transfer the
%% minimum element onto the main tape to its corresponding position. The
%% the heads of the second tape and third tape are moved to the least
%% elements on their lists and this process continues.

\end{proof}

%% ((Is this next definition even required?))

%% \begin{defn}
%%   (Non-deterministic multi-tape Turing machines.) ((Describe the
%%   nondeterministic input $y$, and how the pair $(x,y)$ is checked by
%%   the machine, where the deterministic machine would compute $y$ from
%%   $x$.))
%% \end{defn}

\subsection{Traces of the operation of $\ell$-bit RTMs}

Informally, the \emph{trace} of an $\ell$-bit RTM computation is the
sequence of states that the machine passes through during a
computation, including the relevant portions of the machine's tapes.

\begin{defn}
  (Traces.) The trace of an $\ell$-bit RTM computation is the sequence
  of tuples
  \[
  (t,q_t,a_t,I_t,b_t,J_t,c_t): t < T(n)
  \]
  such that at time $t$,
  \begin{itemize}
    \item the TM is in state $q_t$,
    \item the contents of the address tape are $a_t$,
    \item the address tape's head is positioned at $I_t$,
    \item the contents of the auxiliary tape are $b_t$,
    \item the auxiliary head's position is $J_t$, and
    \item the character under the head of the main tape is $c_t$.
  \end{itemize}
\end{defn}

Note that $a_t$ and $b_t$ are $O(\ell)$ bits long, by the definition
of an $\ell$-bit RTM.

\begin{defn}
  (Consistency of tuples.) A tuple $(t, q', a', I', b', J', c')$ is
  \emph{consistent} with the tuple $(t+1, q, a, I, b, J, c)$ if, when
  the machine is in the state described by the first tuple at time
  $t$, each of $q$, $a$, $I$, $b$ and $J$ \emph{could} result from the
  operation of the machine's transition function given the inputs
  $q'$, $a'$, $I'$, $b'$ and $J'$.
\end{defn}

Note that $c'$ and $c$ are not checked as part of this definition:
they are dealt with separately below.

\subsection{Main result}

\begin{thm}
  $k$-tape nondeterministic TMs can efficiently simulate $\ell$-bit
  RTMs.
\end{thm}

\begin{proof}
  Let $M$ be a frugal RTM and $M'$ be the non-deterministic
  mutli-tape TM on which we are going to simulate the computation on
  $M$. The input $x$ is on one if its tapes. $M'$ guesses traces
  for the computation of $M$ on input $x$, guessing tuples in order
  from $t = 0$ to $T(n)$. Since $M$ is frugal each tuple requires
  guessing a $O(polylog T(n))$ amount of information. This is because

  \begin{itemize}
  \item $t$ is a number between $0$ and $T(n)$ and requires $log T(n)$
    bits
  \item $q_t$ has constant size and only depends on $M$
  \item $a_t$ is of size $O(log T(n)$ by definition
  \item $b_t$ is also of size $O(log T(n)$ by definition
  \item $I_t$ is a number representing a location on the address tape
    and therefore also of size $O(log T(n)$
  \item $I_t$ is a number representing a location on the auxiliary tape
    and therefore also of size $O(log T(n)$
  \item $c_t$ has constant size and depends only on the size of the
    alphabet handled by $M$
  \end{itemize}

  $M'$ giess the 6-tuples one by one and checks that the first tuple
  is correct, and every t+1-st tuple is consistent with the t-th one and
  $q_{T(n)}$ is accepting. The notion of consistency is defined as
  follows

  Note that we \emph{don't} check the $c_{t}$ in each tuple here,
  because of the random-access head movements on the main tape. Once the
  checks here complete, however, we go back and check the $c_{t}$ for
  consistency. To do so,

  \begin{enumerate}
  \item Sort the sequence of tuples first by the contents of the
    address tape, $a_{t}$, and then by time, $t$. (So groups with
    equal $a_{t}$ are formed, and $t$ increases within each group.) By
    lemma 4, we know that this sorting process can be carried out
    efficiently.
  \item For each block (with the same value of $a_{t}$ and increasing
    $t$) we first check if the $c_t$ component of the first tuple is
    the same as the character $M'$ observes under its input
    head. If this is not the case reject.
  \item For two consecutive tuples within the same block, let $c$ be
    the $c_t$ component of the first tuple and $c'$ be the
    corresponding component on the second tuple. Then either of the
    following must be true
    \begin{enumerate}
    \item $c = c{'}$ if the computation corresponding to the first
      tuple did not write any characters onto the main tape
    \item $c \neq c{'}$ but the latter is equal to the character
      written on the main tape by the computation corresponding to the
      first tuple
    \end{enumerate}
    If either of them is not true, then reject.
  \end{enumerate}

  If each group of tuples is consistent, then the entire computation is
  valid and we accept. Also since this consistency check on the
  characters can be done in time proportional to the length of the
  sorted sequence of tuples, the entire process can be carried out
  efficiently.
\end{proof}

\bibliographystyle{plain}
\bibliography{group1}

\end{document}
