\documentclass[english]{article}
\usepackage{mathpazo}
\usepackage{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper]{geometry}
%% \geometry{verbose,tmargin=3cm,bmargin=4cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsthm}
\usepackage{amsmath}

\makeatletter
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{plain}
\newtheorem{lem}[thm]{Lemma}
\makeatother

\usepackage{babel}

%%\setlength{\parindent}{0pt}
%%\setlength{\parskip}{2ex}

\begin{document}

\title{7805 Class project, 2012: Group 1}
\author{
  Tony Garnock-Jones,
  Zahra Jafargholi,
  Ravishankar Rajagopal,\\
  Bochao Shen,
  Saber Shokat Fadaee,
%%  Triết Võ Hữu -- I tried to get inputenc to accept this but it wouldn't
  Triet Vo Huu
}
\maketitle

\section{Overview}

We show that any function computable in time $T(n)$ on a
non-deterministic random-access computer (as defined below) is
computable in time $T(n)(\log T(n))^{O(1)}$ on a non-deterministic
$k$-tape sequential-access Turing machine.\footnote{We restrict
  ourselves to considering situations where $T(n) \geq n$.} We provide
a chain of simulating machines, each of which efficiently simulates
the preceding machine, leading from our initial random-access machine
model to our final sequential-access model.

\begin{defn}
  %% Verbatim text retyped from Gurevich:
  %%
  %% ``... one often checks that any T(n)-time-bounded machine of one
  %% kind can be simulated by some machine of the other kind in time
  %% T(n)h(n) where the overhead h(n) is bounded by a polynomial of
  %% T(n) or even n. ... It is often the case that the overhead h(n)
  %% is bounded by polynomial of the logarithm of T(n); let us call
  %% such simulations efficient.''

  (Efficient simulation, after \cite{DBLP:conf/ershov/GurevichS89}.) A
  machine of some kind that is $T(n)$-time-bounded is said to be
  \emph{efficiently simulated} by some machine of another kind when
  the simulating machine is time-bounded by $T(n)$ multiplied by a
  polynomial of $\log T(n)$; that is, when the overhead of simulation
  is bounded by a polylog function of $T(n)$.
\end{defn}

Although we start from a specific machine model, that of a
random-access computer, it is shown in
\cite{DBLP:conf/ershov/GurevichS89} that a number of variants,
including random-access Turing machines, Kolmogorov machines and
Sch\"{o}nhage machines, all efficiently simulate each other and
therefore share the same notion of $n^k$ computability,
$k\in\mathbb{N}$.

\subsection*{Outline}

We first define the machine models we will be using: random-access
computers (RACs), random-access Turing machines (RTMs), and $k$-tape
sequential-access Turing machines. We then show that every specific
instance of the RAC model can be efficiently simulated by an RTM, and
furthermore that the simulating RTM's behavior is bounded in certain
ways that will be important to the proof of our main claim. Finally,
we move to a non-deterministic setting and show that any computation
performed by a non-deterministic RTM can be efficiently simulated on a
non-deterministic $k$-tape Turing machine, where $k>1$. In order to
accomplish this, we prove and make use of the fact that $k$-tape
Turing machines can sort in time $n \log n$, where $n$ is the length
of the input.

\section{Definitions: RAC, $\ell$-bit RTM, $k$-tape TM}

\begin{defn}
  (Random-access Computer, RAC.) A random-access computer is, quoting
  \cite{DBLP:conf/ershov/GurevichS89}, ``an abstract machine with a
  sequence of memory locations'', where the size of each memory cell
  and the number of such cells varies with the size of the input to
  the machine. In particular, each memory cell is $\ell$ bits wide,
  where $\ell=c \log n$, $c$ is a constant specific to the particular
  machine being considered, and $n$ is the size of the machine's
  input. Furthermore, there are exactly $2^\ell$ memory cells
  available, meaning that the size of each cell is exactly large
  enough to represent any address in the machine's memory space.

  RACs are similar in spirit to the CPUs we use in our day-to-day work
  with desktop computers. Time in an RAC is measured in terms of the
  number of instructions executed. In order to be precise about the
  simulations we will be defining, we choose to pin down a few details
  omitted in \cite{DBLP:conf/ershov/GurevichS89}:

  \begin{itemize}
    \item Our RAC will have a fixed number of $\ell$-bit registers.

    \item The instructions in our RAC will be \textsc{Load} from
      memory to register, \textsc{Store} from register to memory, the
      usual register-to-register arithmetic operations, and control
      transfers. Our primary concern will be with memory accesses, so
      the exact instruction set other than loads and stores is not
      important.
  \end{itemize}
\end{defn}

\begin{defn}
  ($\ell$-bit Random-access Turing Machine, $\ell$-bit RTM.) We start
  from the verbatim\footnote{There are a couple of typos in Gurevich's
    paper; we have corrected them in the presentation here.}
  definition of an RTM from \cite{DBLP:conf/ershov/GurevichS89}:

  \begin{quote}
    An RTM is a Turing machine with three linear tapes called the
    \emph{main tape}, the \emph{address tape} and the \emph{auxiliary
      tape}, such that the head of the main tape (the \emph{main
      head}) is always on the cell whose number is the contents of the
    address tape. An instruction for an RTM has the form
    \begin{equation*}
      (p,\alpha_1,\alpha_2,\alpha_3) \rightarrow
      (q,\beta_1,\beta_2,\beta_3,\gamma_1,\gamma_2)
    \end{equation*}
    and means the following: If the control state is $p$ and the
    symbols in the observed cells on the three tapes are binary digits
    $\alpha_1,\alpha_2,\alpha_3$ respectively, then print binary
    digits $\beta_1,\beta_2,\beta_3$ in the respective cells, move the
    address head to the left (resp. to the right) if $\gamma_1=-1$
    (resp. $\gamma_1=1$), move the auxiliary-tape head with respect
    to $\gamma_2$ and go to control state $q$.
  \end{quote}

  We extend Gurevich and Shelah's definition with the constraint that
  both the address and the auxiliary tapes are bounded so as to be no
  longer than $O(\ell)$ bits each.\footnote{Note that this is
    different to the class of \emph{frugal} RTMs, defined as part of
    the chain of machines in \cite{DBLP:conf/ershov/GurevichS89}. The
    lengths of the address and auxiliary tapes in a frugal RTM are
    bounded by $O(\log t)$ at every moment in the operation of the
    machine; we do not rely on such a strict bound here, so do not
    further consider frugal RTMs as defined by Gurevich and Shelah.}
\end{defn}

\begin{defn}
  ($k$-tape sequential-access Turing machines, $k$-tape TMs.) A
  $k$-tape TM is a Turing machine with $k$ linear tapes, each of which
  has a corresponding head, which is constrained to move one cell at a
  time along its tape, just like a normal single-tape Turing
  machine. Instructions for $k$-tape TMs have the form
  \begin{equation*}
    (p,\alpha_1,\alpha_2,...\alpha_k) \rightarrow
    (q,\beta_1,\beta_2,...\beta_k,\gamma_1,\gamma_2,...\gamma_k)
  \end{equation*}
  where $p$ is the control state, the $\alpha$s are the contents of
  the cells under the heads of the $k$ tapes, $q$ is the resulting
  control state, the $\beta$s are to be written into the cells under
  the heads, and the $\gamma$s control each head's movements in the
  same way as they do for RTMs as defined above.
\end{defn}

\section{Efficient simulation of RACs using $\ell$-bit RTMs}

\begin{lem}
  RACs with $\ell$-bit memory cells can be efficiently simulated by
  $\ell$-bit RTMs.
\end{lem}

\begin{proof}
  Since RACs with $\ell$-bit memory cells also have exactly $2^\ell$
  such cells, we simulate the action of the RAC's memory with the main
  tape of our RTM. The main tape is logically divided into $2^\ell$
  $\ell$-bit-wide sections, each corresponding to one memory cell in
  the simulated RAC. The auxiliary tape of the RTM is used to hold the
  registers of the RAC and also to perform arithmetic involved in the
  execution of RAC instructions.

  The RAC's instructions are then encoded into the state transition
  function of the RTM. Control transfer instructions can be encoded
  directly in the state transition function, and register-to-register
  arithmetic and logic instructions can be performed in $O(\ell)$
  space and $O(\ell^2)$ time on the auxiliary tape. The remaining
  instructions are \textsc{Load} and \textsc{Store}.

  Each \textsc{Load} instruction takes $O(\ell)$ time to copy from a
  register held on the auxiliary tape to the address tape. Once the
  copy is completed, $\ell$ adjacent bits must be copied from the main
  tape to the auxiliary tape, which involves $\ell$ transfers of a
  single bit and $\ell$ incrementations of the value held on the
  address tape. Transferring one bit takes worst-case $O(\ell)$ time
  for the head to move back and forth to the right location on the
  auxiliary tape, since the tape itself has length bounded by
  $O(\ell)$, and incrementing the address tape's value takes
  worst-time $O(\ell)$ steps, so overall the process of copying the
  $\ell$ bits from the main to the auxiliary tape takes a total of
  $O(\ell^2)$ time. In summary, then the \textsc{Load} instruction as
  a whole takes time bounded by $O(\ell^2)$. The time bound for
  \textsc{Store} instructions is the same, since the copying of the
  bits simply takes place in reverse.

  Since every class of instruction can be completed in $O(\ell^2)$
  time, we see that the simulation completes in $O(T(n)
  \ell^2)$. Recalling that $\ell=c \log n$, and $T(n) \geq n$ by
  assumption, we see that this is equivalent to saying the simulation
  completes in $O(T(n) (c \log T(n))^2)$, which is itself equivalent
  to $T(n)(\log T(n))^{O(1)}$, which is what is required for the
  definition of efficient simulation to apply.
\end{proof}

\section{Efficient simulation of non-deterministic $\ell$-bit RTMs using non-deterministic $k$-tape TMs}

Simulating a computation on a non-deterministic $\ell$-bit RTM using a
non-deterministic multi-tape TM is carried out by using the
non-deterministic input to the latter to \emph{guess} the sequence of
steps involved in the former's computation and then checking the
correctness of the guess. However, in order to do the latter, we need
to show that multi-tape TMs can sort efficiently.

We proceed in this section by first getting the question of efficient
sorting out of the way, and then being more precise about the kind of
non-determinism we are using. We then formally define the way the
simulating machine represents the steps taken by the simulated RTM,
and finally we bring all the pieces together to show our main result.

\subsection{Efficient sorting}

This section is based on a lemma given in
\cite{Schnorr:1978:SQC:322047.322060} as part of a proof that the
graph isomorphism problem is in NQL.

\begin{lem}
  \label{efficient-sorting-lemma}
  Let $L$ be a list containing $|L|$ natural numbers, and let $C(L)$
  be an encoding of $L$, of length $|C(L)|$ symbols. Then, $L$ can be
  sorted by a $k$-tape TM in time $O(|C(L)|\log|L|)$.
\end{lem}

\begin{proof}
  In the following let $[n] = \{1,2,\ldots n\}$. Let $L = (l_i \in N:
  i \in [m])$ be a list (sequence) of natural numbers $(l_1,l_2,
  \ldots l_m)$. Then the encoding $C(L)$ $\in \Sigma^*$ is defined as
\[
C(L) = c(l_1) \# c(l_2) \# \ldots c(l_m) \#
\]
  where $\Sigma$, the input alphabet to our sorting machine, is the
  set including $0$ and $1$ as well as the pound symbol ($\#$), and
  $c(v)$ is the binary representation of $v \in \mathbb{N}$. For
  example, $C((1,2,13))=1\#10\#1101\#$.

  We define $\Gamma$, our tape alphabet, to be $\Sigma$ with the
  addition of the symbol comma ($,$). We proceed using an algorithm
  based on merge sort, where we repeatedly merge adjacent sorted
  sublists until just one list remains. At the $i$th iteration over
  the input, the sublists we consider have length $2^{i-1}$.

  We choose to use three tapes: the first, main, tape holds the input
  and the partial outputs at each iteration, and the second and third
  tapes are used as working space to hold the sublists to be merged. \\

  The operation of the machine in each iteration is as follows:

  \begin{enumerate}
    \item \label{iteration-start} Return the heads to the beginnings
      of the tapes.
    \item \label{process-sublist} If we are at the end of the main
      tape, then
      \begin{itemize}
        \item if we haven't yet processed any sublists during this
          iteration, the original input was empty. Halt, since an
          empty list is always sorted.
        \item otherwise, go to step \ref{iteration-start}.
      \end{itemize}
    \item Copy symbols from the main tape to the second tape until a
      pound symbol is seen.
    \item If the end of the tape follows the pound symbol we just saw,
      then
      \begin{itemize}
        \item if this was the first sublist we processed during this
          iteration, then we are done: the input has been
          sorted. Halt.
        \item otherwise, we have an odd number of sublists. The final
          iteration of the algorithm will take care of this
          case. Return to step \ref{iteration-start}.
      \end{itemize}
    \item Otherwise, copy symbols from the main tape to the third tape
      until a pound symbol is seen.
    \item Return the heads of the second and third tapes to the
      beginning, and move the head of the main tape back over the two
      sublists just copied to the first symbol of the first sublist of
      the two.
    \item \label{merge-loop} If the symbol on the second tape is the
      pound symbol, place a comma on the main tape and then copy
      symbols from the third tape onto the main tape until we see a
      pound symbol on the third tape also. Place a pound symbol on the
      main tape and go to step \ref{process-sublist}.
    \item Otherwise, if the symbol on the third tape is the pound
      symbol, place a comma on the main tape and then copy symbols
      from the second tape onto the main tape until we see a pound
      symbol on the second tape also. Place a pound symbol on the main
      tape and go to step \ref{process-sublist}.
    \item Otherwise, compare the encodings of the numbers under the
      heads of the second and third tapes. If the smaller is the
      element from the second tape, copy it to the main tape;
      otherwise, copy the element from the third tape. Place a comma
      on the main tape. Rewind whichever of the tapes did not have an
      element copied from it so that its element will be compared
      again next time round. Go to step \ref{merge-loop}.
  \end{enumerate}

  If there are an even number of pound-symbol-delimited sublists in
  the input, then they are of equal length at the start of each
  iteration; and if there are an odd number in the input, then all but
  one will have equal length. To see this, consider that as the
  algorithm begins, all sublists are of length 1, and as it proceeds,
  each pair of length $x$ sublists is replaced by a single length $2x$
  sublist, except for any odd-man-out at the end of the input, which
  will not be processed until the final iteration.

  It remains to be shown that the machine takes time
  $O(|C(L)|\log|L|)$. We can see that there will be $log_2|L|$
  iterations, since each iteration halves the number of remaining
  sublists until a single sublist remains. Each iteration will take
  $O(|C(L)|)$ steps, since every element of the input is examined on
  each iteration, and $O(1)$ steps are needed to perform the necessary
  copyings and comparisons for each element of the input. Combining
  these two observations gives us the required bound.
\end{proof}

%% Each comparison process goes as follows: Based on the count of the
%% stages from the fourth tape we copy the first $2^{n-1}$ onto tape two
%% and the next $2^{n-1}$ elements onto tape three. Then we compare the
%% first two elements on the two tapes bit by bit and transfer the
%% minimum element onto the main tape to its corresponding position. The
%% the heads of the second tape and third tape are moved to the least
%% elements on their lists and this process continues.

\subsection{Non-determinism}

\begin{defn}
  We say Machine M \emph{non-deterministically accepts} language $L$,
  if
  \begin{itemize}
    \item $\forall x \in L$, $\exists y$ whose size is bounded by
      $n^{k_1} (\log n)^{k_2}$ for some fixed constants $k_1$ and
      $k_2$ such that $M(x,y) = 1$, and
    \item $\forall x \notin L$, $\forall y$ whose size is bounded as
      given above, $M(x,y) = 0$.
  \end{itemize}
\end{defn}

\subsection{Traces of the operation of $\ell$-bit RTMs}

Informally, the \emph{trace} of an $\ell$-bit RTM computation is the
sequence of states that the machine passes through during a
computation, including the relevant portions of the machine's tapes.

\begin{defn}
  (Traces and trace tuples.) The trace of an $\ell$-bit RTM
  computation is the sequence of trace tuples
  \[
  (t,q_t,a_t,I_t,b_t,J_t,c_t): t < T(n)
  \]
  such that at time $t$, the TM is in state $q_t$, the contents of the
  address tape are $a_t$, the address tape's head is positioned at
  $I_t$, the contents of the auxiliary tape are $b_t$, the auxiliary
  head's position is $J_t$, and the character under the head of the
  main tape is $c_t$.
\end{defn}

Note that $a_t$ and $b_t$ are each $O(\ell)$ bits long, by the
definition of an $\ell$-bit RTM.

\begin{defn}
  (Consistency of trace tuples.) A tuple $(t, q', a', I', b', J', c')$
  is \emph{consistent} with the tuple $(t+1, q, a, I, b, J, c)$ if,
  when the machine is in the state described by the first tuple at
  time $t$, each of $q$, $a$, $I$, $b$ and $J$ \emph{could} result
  from the operation of the machine's transition function given the
  inputs $q'$, $a'$, $I'$, $b'$ and $J'$.
\end{defn}

Note that $c'$ and $c$ are not checked as part of this definition:
they are dealt with separately below.

\subsection{Main result}

\begin{thm}
  $k$-tape nondeterministic TMs can efficiently simulate
  non-deterministic $\ell$-bit RTMs.
\end{thm}

\begin{proof}
  Let $M$ be an $\ell$-bit RTM and $M'$ be the non-deterministic
  $k$-tape TM intended to simulate the action of $M$. The input
  $(x,y)$, incorporating both the original input $x$ and the
  non-deterministic contribution $y$ to $M$, is placed on one of the
  tapes of $M'$, along with a further non-deterministic contribution
  $y'$. $M'$ uses $y'$ to construct a guess of a possible trace for
  the computation of $M$ on input $(x,y)$, guessing tuples in order
  from $t = 0$ through $t = T(n)$. Since $M$'s address and auxiliary
  tapes are bounded in size by $O(\ell)$, and $\ell$ is bounded by a
  polylog function of $T(n)$, each tuple to be guessed consumes at
  most polylog $T(n)$ bits of information from $y'$. In other words,

  \begin{itemize}
  \item $t$ is a number between $0$ and $T(n)$ and therefore requires
    $\log T(n)$ bits
  \item $q_t$ has constant size and depends only on the definition of
    $M$
  \item $a_t$ has size bounded by $O(\log T(n))$ by definition
  \item $b_t$ is also bounded by $O(\log T(n))$ by definition
  \item $I_t$ is a number representing a location on the address tape,
    at most $O(\log T(n))$, and so is of size $O(\log \log T(n))$
  \item $J_t$ is a number representing a location on the auxiliary
    tape, therefore is also of size $O(\log \log T(n))$
  \item $c_t$ has constant size and depends only on the size of the
    alphabet handled by $M$
  \end{itemize}
  so the overall size of each tuple is bounded by $O(\log T(n))$.

  $M'$ guesses the trace tuples one by one, performing an initial
  correctness-check as it goes by making sure that the first tuple is
  correct, that each subsequent tuple is consistent with its
  predecessor, and finally that the last tuple in the trace has
  $q_{T(n)}$ an accepting state.

  Recall that when we defined trace tuple consistency above, we
  explicitly neglected to check the $c_t$ in each tuple. The reason
  for this is that the main tape's head accesses the main tape in a
  random-access fashion rather than sequentially. Once the initial
  pass over the generated trace is complete, however, we go back and
  check the $c_{t}$ for consistency. To do so,

  \begin{enumerate}
  \item Sort the trace first by the contents of the address tape,
    $a_t$, and then by time, $t$. After this process, groups with
    equal $a_t$ will exist, with each tuple's $t$ increasing within
    each group. By lemma \ref{efficient-sorting-lemma}, we know that
    this sorting process can be carried out efficiently.
  \item For each block with the same value of $a_t$ and increasing
    $t$, we first check that the $c_t$ component of the first tuple is
    the same as the character $M'$ observes on its copy of the
    original input to the machine at address $a_t$. If this is not the
    case, our trace is inconsistent with the operation of $M$.
  \item For two adjacent tuples within the same block, let $c$ be the
    $c_t$ component of the first tuple and $c'$ be the corresponding
    component on the second tuple. Then, for the pair of tuples to be
    consistent with each other, one of the following conditions must
    hold:
    \begin{enumerate}
    \item $c = c'$ and the computation following from application of
      $M$'s transition function to the first tuple would not have
      altered the main tape at $a_t$; or
    \item $c \neq c'$, and the transition function applied to the
      first tuple would have written exactly $c'$ at $a_t$.
    \end{enumerate}
  \end{enumerate}

  If each $a_t$-group of tuples is consistent, then the entire trace
  represents a legitimate accepting execution of $M$, and so $M'$
  accepts.

  Since we generate the trace in time $T(n)$, perform the first
  consistency-check pass in $O(T(n))$, sort the trace in $O(T(n) \log
  T(n))$, and perform the second consistency-check pass in $O(T(n))$,
  we see that the overall process has time bounded by $O(T(n) \log
  T(n))$, which is a polylog overhead compared to $T(n)$, and so the
  whole process is an efficient simulation.
\end{proof}

\bibliographystyle{plain}
\bibliography{group1}

\end{document}
