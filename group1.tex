\documentclass[english]{article}
\usepackage{mathpazo}
\usepackage{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=3cm,bmargin=4cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{natbib}

\makeatletter
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{plain}
\newtheorem{lem}[thm]{Lemma}
\makeatother

\usepackage{babel}

%%\setlength{\parindent}{0pt}
%%\setlength{\parskip}{2ex}

\begin{document}

\title{7805 Class project, 2012: Group 1}
\author{
  Tony Garnock-Jones,
  Zahra Jafargholi,
  Ravishankar Rajagopal,\\
  Bochao Shen,
  Saber Shokat Fadaee,
%%  Triết Võ Hữu -- I tried to get inputenc to accept this but it wouldn't
  Triet Vo Huu
}
\maketitle

\section{Overview}

We show that any function computable in time $T(n)$ on a
nondeterministic random-access Turing machine (RTM) is computable in
time $T(n)(\log T(n))^{O(1)}$ on a $k$-tape nondeterministic
non-random-access Turing machine.\footnote{$T(n)(\log T(n))^{O(1)}$
  doesn't quite line up with the definition of efficient simulation
  below! It's stricter; can we actually achieve this stricter bound?
  Or should we relax our goal to be polylog-overhead, as per the
  definition of efficient simulation?}
The core idea is to efficiently simulate some machine model that
allows random access using a model lacking random access.

\begin{defn}
  %% Verbatim text retyped from Gurevich:
  %%
  %% ``... one often checks that any T(n)-time-bounded machine of one
  %% kind can be simulated by some machine of the other kind in time
  %% T(n)h(n) where the overhead h(n) is bounded by a polynomial of
  %% T(n) or even n. ... It is often the case that the overhead h(n)
  %% is bounded by polynomial of the logarithm of T(n); let us call
  %% such simulations efficient.''

  (Efficient simulation, after \cite{DBLP:conf/ershov/GurevichS89}.) A
  machine of some kind that is $T(n)$-time-bounded is said to be
  \emph{efficiently simulated} by some machine of another kind when
  the simulating machine is time-bounded by a polynomial of $\log
  T(n)$; that is, when the overhead of simulation is bounded by a
  polylog function of $n$.
\end{defn}

Although we start from a specific machine, RTM (defined formally
below), it is shown in \cite{DBLP:conf/ershov/GurevichS89} that a
number of variants, namely RAC (Random Access Computer), Kolmogorov
machines and Sch\"{o}nhage machines, all efficiently simulate each
other and therefore share the same notion of $n^{O(1)}$ computability.

We first define the notion of a non-deterministic RTM. We then define
a variant of RTM called a frugal RTM, and show that every specific RTM
can be efficiently simulated by some frugal RTM. Finally, we show how
any computation using a \emph{non-deterministic} frugal RTM can be
efficiently simulated on a $k$-tape non-deterministic Turing machine,
where $k>1$. In order to accomplish this, we prove and make use of the
fact that a non-deterministic Turing machine can sort in time $n \log
n$, where $n$ is the length of the input.

\section{From RTMs to Frugal RTMs}

\begin{defn}
  (Random-access Turing Machine, RTM.) We use verbatim\footnote{There
    are a couple of typos in Gurevich's paper; we have corrected them
    in the presentation here.} the definition from
  \cite{DBLP:conf/ershov/GurevichS89}:

  \begin{quote}
    An RTM is a Turing machine with three linear tapes called the
    \emph{main tape}, the \emph{address tape} and the \emph{auxiliary
      tape}, such that the head of the main tape (the \emph{main
      head}) is always on the cell whose number is the contents of the
    address tape. An instruction for an RTM has the form
    \begin{equation*}
      (p,\alpha_1,\alpha_2,\alpha_3) \rightarrow
      (q,\beta_1,\beta_2,\beta_3,\gamma_1,\gamma_2)
    \end{equation*}
    and means the following: If the control state is $p$ and the
    symbols in the observed cells on the three tapes are binary digits
    $\alpha_1,\alpha_2,\alpha_3$ respectively, then print binary
    digits $\beta_1,\beta_2,\beta_3$ in the respective cells, move the
    address head to the left (resp. to the right) if $\gamma_1=-1$
    (resp. $\gamma_1=1$), move the auxiliary-tape head with respect
    to $\gamma_2$ and go to control state $q$.
  \end{quote}
\end{defn}

\begin{defn}
  (Frugal RTM.) An RTM is called \emph{frugal} if at any time $t$, the
  lengths of the address tape and of the auxiliary tape are both
  bounded by $O(\log t)$.
\end{defn}

\begin{thm}
  Frugal RTMs can efficiently simulate RTMs.
\end{thm}

\begin{proof}
  ((I'll write this up ---tonyg))
\end{proof}

\section{From Nondeterministic Frugal RTMs to Nondeterministic Multi-tape TMs}

In the previous section, we had been considering deterministic Turing
machines. We now switch to considering their non-deterministic
counterparts.

\begin{defn}
  (Non-deterministic Turing machines.) ((Describe the nondeterministic
  input $y$, and how the pair $(x,y)$ is checked by the machine, where
  the deterministic machine would compute $y$ from $x$.))
\end{defn}

Simulating a computation on a frugal RTM using a multi-tape TM is
carried out by non-deterministically guessing the sequence of steps on
the computation and then check for the correctness of the
sequence. However in order to do the later, we need multi-tape TMs to
be able to sort efficiently.

\begin{lem}
  Let $\ell$ be a list of natural numbers, and let $C(\ell)$ be the
  binary encoding of $\ell$. Then, $\ell$ can be sorted in time
  $C(\ell)\log|\ell|$.
\end{lem}

\begin{proof}
  In the following let $[n]$ = $\{1,2,\ldots n\}$. Let $\ell$ = $(l_i
  \in N: i \in [m])$ be a list(sequence of natural numbers $(l_1,l_2,
  \ldots l_m)$. Then the binary encoding $C(\ell)$ $\in K^*$ is
  defined as
\[
C(\ell) = \overline{c(l_1)},\overline{c(l_2)}, \ldots \overline{c(l_m)}
\]

  where $c(v)$ is the binary representation of $v \in \mathbb{N}$ and
  $\overline{x}$ is the doubled sequence which is associated with x
  (E.g. if x = 101 then $\overline{x}$ = 110011).This is done in order
  to identify where one number ends and the next begins. Let $|\ell|$
  be the number of elements in the list $\ell$.\\

  We use a procedure similar to merge sort in a number of stage. In
  the first stage we sort the first and second element, the third and
  fourth and so on. In the second state we sort the first four
  elements, the next four and so on. Thus at stage $n$ we sort
  sequences of length $2^n$ which is made easier by the fact that the
  first $2^{n-1}$ and similarly the next $2^{n-1}$ elements are alread
  sorted. We assume the existence of four tapes which are used as
  follows:\\

  First Tape - Holds the input and the partial outputs at every stage

  Second Tape - Holds the first $2^{n-1}$ elements when at stage $n$

  Third Tape - Holds the second $2^{n-1}$ elements when at stage $n$

  Fourth Tape - Keeps a count of the stages so that we know the length
  of the elements to be copied over to the second and the third
  tapes.This also holds the length of each element on the list.\\

  Each comparison process goes as follows: Based on the count of the
  stages from the fourth tape we copy the first $2^{n-1}$ onto tape
  two and the next $2^{n-1}$ elements onto tape three. Then we compare
  the first two elements on the two tapes bit by bit and transfer the
  minimum element onto the main tape to its corresponding
  position. The the heands of the second tape and third tape are moved
  to the least elements on their lists and this process continues.\\

  More formally in stage $i$ we sort the segments

\[
\ell(k,i) = \{l_j: k2^i \leq j < k2^i + 2^i\} for k \leq |\ell|/2^i
\]

  by merging the ordered segments $\ell(2k,i - 1),\ell(2k + 1,i - 1)$
  which have been generated at stage $i - 1$. This requires $O(log
  |\ell|)$ stages beginning with stage 1. Since merging of to lists
  can be done within linear time, each stage can be done within
  $O(|C(\ell)|)$ steps.
\end{proof}

We first define the trace of a random access TM computation which
informally is the sequence of states that the machine is in during a
computation.

\begin{defn}
  The trace T of a random-access TM computation is the sequence of tupes
\[
(t,q_t,a_t,I_t,b_t,J_t,c_t): t < T(n)
\]
  such that at time $t$, the TM is in state $q_t$, the address tape
  contents is $a_t$ with the head at $I_t$, the auxiliary tape
  contents is $b_t$ with the head at $J_t$ and the character under the
  head of the main tape is $c_t$.
\end{defn}

\begin{thm}
  $k$-tape nondeterministic TMs can efficiently simulate
  non-deterministic Frugal RTMs.
\end{thm}

\begin{proof}
  Let $M$ be a frugal RTM and$M^{'}$ be the non-deterministic
  mutli-tape TM on which we are going to simulate the computation on
  $M$. The input $x$ is on one if its tapes. $M^{'}$ guesses traces
  for the computation of $M$ on input $x$, guessing tuples in order
  from $t = 0$ to $T(n)$. Since $M$ is frugal each tuple requires
  guessing a $O(polylog T(n))$ amount of information. This is because

  \begin{itemize}
  \item $t$ is a number between $0$ and $T(n)$ and requires $log T(n)$
    bits
  \item $q_t$ has constant size and only depends on $M$
  \item $a_t$ is of size $O(log T(n)$ by definition
  \item $b_t$ is also of size $O(log T(n)$ by definition
  \item $I_t$ is a number representing a location on the address tape
    and therefore also of size $O(log T(n)$
  \item $I_t$ is a number representing a location on the auxiliary tape
    and therefore also of size $O(log T(n)$
  \item $c_t$ has constant size and depends only on the size of the
    alphabet handled by $M$
  \end{itemize}

  $M^{'}$ giess the 6-tuples one by one and checks that the first tuple
  is correct, and every t+1-st tuple is consistent with the t-th one and
  $q_{T(n)}$ is accepting. The notion of consistency is defined as
  follows

  \begin{defn}
    A tuple $(t^{'}, q^{'}, a^{'}, I^{'}, b^{'}, J^{'}, c^{'})$ is
    consistent with the tuple $(t, q, a, I, b, J, c)$ if when in state q
    with address tape a with head at position I, auxiliary tape B with
    head at position J and observing character c on the main tape, $M$
    transitions to state $q^{'}$, updates the address tape to $a^{'}$
    and moves its head to position $I^{'}$, updates the auxiliary tape
    to $b^{'}$ and moves its head to position $J^{'}$.
  \end{defn}

  Note that we \emph{don't} check the $c_{t}$ in each tuple here,
  because of the random-access head movements on the main tape. Once the
  checks here complete, however, we go back and check the $c_{t}$ for
  consistency. To do so,

  \begin{enumerate}
  \item Sort the sequence of tuples first by the contents of the
    address tape, $a_{t}$, and then by time, $t$. (So groups with
    equal $a_{t}$ are formed, and $t$ increases within each group.) By
    lemma 4, we know that this sorting process can be carried out
    efficiently.
  \item For each block (with the same value of $a_{t}$ and increasing
    $t$) we first check if the $c_t$ component of the first tuple is
    the same as the character $M^{'}$ observes under its input
    head. If this is not the case reject.
  \item For two consecutive tuples within the same block, let $c$ be
    the $c_t$ component of the first tuple and $c^{'}$ be the
    corresponding component on the second tuple. Then either of the
    following must be true
    \begin{enumerate}
    \item $c = c{'}$ if the computation corresponding to the first
      tuple did not write any characters onto the main tape
    \item $c <> c{'}$ but the later is equal to the character
      written on the main tape by the computation corresponding to
      the first tuple
    \end{enumerate}
    If either of them is not true, then reject.
  \end{enumerate}

  If each group of tuples is consistent, then the entire computation is
  valid and we accept. Also since this consistency check on the
  characters can be done in time proportional to the length of the
  sorted sequence of tuples, the entire process can be carried out
  efficiently.
\end{proof}

\bibliographystyle{plainnat}
\bibliography{group1}

\end{document}
